{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNMSai5ze/hO3GT1Gs4T76m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Question 2\n","\n","\n","\n","\n","\n"],"metadata":{"id":"GMP7wzlJYSqn"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"hvE7KifjYKH4","executionInfo":{"status":"ok","timestamp":1734116241125,"user_tz":300,"elapsed":15878,"user":{"displayName":"ERIC CHEUNG","userId":"17439827945453231901"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.optim as optim\n","\n","# A modified version of LeNet5 with modern tweaks\n","class ModifiedLeNet5(nn.Module):\n","    def __init__(self):\n","        super(ModifiedLeNet5, self).__init__()\n","        # Input is 32x32\n","        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1)\n","        # Max pool replaces average pool, kernel_size=2\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n","        # After second conv and pool, we flatten and go to FC layers\n","\n","        # C5 and F6 replaced with standard fc layers:\n","        # After conv layers:\n","        # Input: 32x32 -> conv1(6@28x28) -> pool -> 6@14x14\n","        # conv2 -> 16@10x10 -> pool -> 16@5x5\n","        # Flatten: 16*5*5 = 400\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","        # Add dropout for better generalization\n","        self.dropout = nn.Dropout(p=0.5)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)  # We will apply softmax in the loss function, so no final activation\n","        return x"]},{"cell_type":"code","source":["\n","import torch.optim as optim\n","from PIL import Image\n","import io\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchvision import transforms\n","import torch.nn.functional as TorchFunc\n","\n","import pandas as pd\n","from PIL import Image\n","\n","\n","\n","splits = {'train': 'mnist/train-00000-of-00001.parquet', 'test': 'mnist/test-00000-of-00001.parquet'}\n","df_train = pd.read_parquet(\"hf://datasets/ylecun/mnist/\" + splits[\"train\"])\n","df_test = pd.read_parquet(\"hf://datasets/ylecun/mnist/\" + splits[\"test\"])\n","\n","\n","# Data augmentation transforms\n","# Here we add random rotations, shifts, and slight scaling\n","train_transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.9,1.1)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))  # Normalizing MNIST\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","# Assuming df_train and df_test are loaded as in your original code\n","# Modify your get_images function to return PIL images directly, then apply transforms\n","# Or simply adapt to run transforms here.\n","from PIL import Image\n","import io\n","\n","def get_dataset(df, transform):\n","    images = []\n","    labels = []\n","    for row in df.itertuples():\n","        img = Image.open(io.BytesIO(row[1]['bytes'])).convert('L')  # Ensure grayscale\n","        img = transform(img)\n","        images.append(img)\n","        labels.append(row[2])\n","\n","    images = torch.stack(images)\n","    labels = torch.tensor(labels, dtype=torch.long)\n","    dataset = TensorDataset(images, labels)\n","    return dataset\n","\n","train_dataset = get_dataset(df_train, train_transform)\n","test_dataset = get_dataset(df_test, test_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Initialize model, loss, optimizer\n","model = ModifiedLeNet5().to('cuda' if torch.cuda.is_available() else 'cpu')\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","def train(model, loader, optimizer, criterion):\n","    model.train()\n","    running_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    device = next(model.parameters()).device\n","\n","    for data, targets in loader:\n","        data, targets = data.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * data.size(0)\n","        _, predicted = outputs.max(1)\n","        total_correct += predicted.eq(targets).sum().item()\n","        total_samples += data.size(0)\n","\n","    avg_loss = running_loss / total_samples\n","    accuracy = total_correct / total_samples\n","    return avg_loss, accuracy\n","\n","def test(model, loader, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    device = next(model.parameters()).device\n","\n","    with torch.no_grad():\n","        for data, targets in loader:\n","            data, targets = data.to(device), targets.to(device)\n","            outputs = model(data)\n","            loss = criterion(outputs, targets)\n","\n","            running_loss += loss.item() * data.size(0)\n","            _, predicted = outputs.max(1)\n","            total_correct += predicted.eq(targets).sum().item()\n","            total_samples += data.size(0)\n","\n","    avg_loss = running_loss / total_samples\n","    accuracy = total_correct / total_samples\n","    return avg_loss, accuracy\n","\n","# Training loop\n","epochs = 20\n","for epoch in range(1, epochs + 1):\n","    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n","    test_loss, test_acc = test(model, test_loader, criterion)\n","    print(f\"Epoch {epoch}/{epochs}, Train Loss: {train_loss:.4f}, \"\n","          f\"Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHbXE9eJYXbw","executionInfo":{"status":"ok","timestamp":1734116638836,"user_tz":300,"elapsed":397722,"user":{"displayName":"ERIC CHEUNG","userId":"17439827945453231901"}},"outputId":"845d860f-1434-4c64-8bf3-a5558b52566f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Train Loss: 0.7391, Train Acc: 0.7611, Test Acc: 0.9727\n","Epoch 2/20, Train Loss: 0.3086, Train Acc: 0.9125, Test Acc: 0.9823\n","Epoch 3/20, Train Loss: 0.2346, Train Acc: 0.9345, Test Acc: 0.9872\n","Epoch 4/20, Train Loss: 0.1959, Train Acc: 0.9450, Test Acc: 0.9879\n","Epoch 5/20, Train Loss: 0.1750, Train Acc: 0.9510, Test Acc: 0.9879\n","Epoch 6/20, Train Loss: 0.1561, Train Acc: 0.9574, Test Acc: 0.9885\n","Epoch 7/20, Train Loss: 0.1439, Train Acc: 0.9600, Test Acc: 0.9901\n","Epoch 8/20, Train Loss: 0.1351, Train Acc: 0.9611, Test Acc: 0.9869\n","Epoch 9/20, Train Loss: 0.1251, Train Acc: 0.9651, Test Acc: 0.9912\n","Epoch 10/20, Train Loss: 0.1195, Train Acc: 0.9664, Test Acc: 0.9919\n","Epoch 11/20, Train Loss: 0.1117, Train Acc: 0.9680, Test Acc: 0.9904\n","Epoch 12/20, Train Loss: 0.1078, Train Acc: 0.9691, Test Acc: 0.9912\n","Epoch 13/20, Train Loss: 0.1034, Train Acc: 0.9694, Test Acc: 0.9908\n","Epoch 14/20, Train Loss: 0.0982, Train Acc: 0.9720, Test Acc: 0.9922\n","Epoch 15/20, Train Loss: 0.0941, Train Acc: 0.9730, Test Acc: 0.9876\n","Epoch 16/20, Train Loss: 0.0922, Train Acc: 0.9731, Test Acc: 0.9907\n","Epoch 17/20, Train Loss: 0.0849, Train Acc: 0.9753, Test Acc: 0.9904\n","Epoch 18/20, Train Loss: 0.0872, Train Acc: 0.9751, Test Acc: 0.9914\n","Epoch 19/20, Train Loss: 0.0863, Train Acc: 0.9744, Test Acc: 0.9901\n","Epoch 20/20, Train Loss: 0.0819, Train Acc: 0.9756, Test Acc: 0.9913\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the save path in Google Drive\n","save_path = \"/content/drive/MyDrive/MLHW4/LeNet_2.pth\"\n","\n","# Save the model's state dictionary\n","torch.save(model.state_dict(), save_path)\n","\n","print(f\"Model saved to {save_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q87e0b9NbLXM","executionInfo":{"status":"ok","timestamp":1734116992718,"user_tz":300,"elapsed":14711,"user":{"displayName":"ERIC CHEUNG","userId":"17439827945453231901"}},"outputId":"6e0aced7-0ecd-4fcf-9b61-fee46222b6e7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Model saved to /content/drive/MyDrive/MLHW4/LeNet_2.pth\n"]}]}]}